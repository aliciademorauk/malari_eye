{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3314979a-c6ad-4dfb-9f11-93f061a137b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"id\": \"c22687dd-2b31-460f-8d34-1bbf0851675f\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"2023-03-13 17:42:31.433495: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\\n\",\n",
    "      \"To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\\n\",\n",
    "      \"2023-03-13 17:42:32.293323: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\\n\",\n",
    "      \"2023-03-13 17:42:32.293442: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\\n\",\n",
    "      \"2023-03-13 17:42:32.293454: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import tensorflow.keras as K\\n\",\n",
    "    \"import tensorflow as tf\\n\",\n",
    "    \"import cv2\\n\",\n",
    "    \"from PIL import Image\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 2,\n",
    "   \"id\": \"a7bcd647-473d-49f3-82c4-1c0212525783\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open('data/malaria/training.json') as f_in:\\n\",\n",
    "    \"    data_train = json.load(f_in)\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open('data/malaria/test.json') as f_in:\\n\",\n",
    "    \"    data_test = json.load(f_in)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 3,\n",
    "   \"id\": \"3577ddbd-17b0-4ae2-8380-6b39640f78a0\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"cells_train_df = {\\n\",\n",
    "    \"    'path':[],\\n\",\n",
    "    \"    'min_r':[],\\n\",\n",
    "    \"    'min_c':[],\\n\",\n",
    "    \"    'max_r':[],\\n\",\n",
    "    \"    'max_c':[],\\n\",\n",
    "    \"    'r_len':[],\\n\",\n",
    "    \"    'c_len':[],\\n\",\n",
    "    \"    'category':[]\\n\",\n",
    "    \"}\\n\",\n",
    "    \"for image in data_train:    \\n\",\n",
    "    \"    #Populate cells_df dict\\n\",\n",
    "    \"    for box in image['objects']:\\n\",\n",
    "    \"        cells_train_df['path'].append(image['image']['pathname'])\\n\",\n",
    "    \"        cells_train_df['min_r'].append(box['bounding_box']['minimum']['r'])\\n\",\n",
    "    \"        cells_train_df['min_c'].append(box['bounding_box']['minimum']['c'])\\n\",\n",
    "    \"        cells_train_df['max_r'].append(box['bounding_box']['maximum']['r'])\\n\",\n",
    "    \"        cells_train_df['max_c'].append(box['bounding_box']['maximum']['c'])\\n\",\n",
    "    \"        cells_train_df['r_len'].append(box['bounding_box']['maximum']['r']-box['bounding_box']['minimum']['r'])\\n\",\n",
    "    \"        cells_train_df['c_len'].append(box['bounding_box']['maximum']['c']-box['bounding_box']['minimum']['c'])\\n\",\n",
    "    \"        cells_train_df['category'].append(box['category'])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 4,\n",
    "   \"id\": \"f8f40646-00b9-4b93-8aa2-b36f361a302b\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"cells_test_df = {\\n\",\n",
    "    \"    'path':[],\\n\",\n",
    "    \"    'min_r':[],\\n\",\n",
    "    \"    'min_c':[],\\n\",\n",
    "    \"    'max_r':[],\\n\",\n",
    "    \"    'max_c':[],\\n\",\n",
    "    \"    'r_len':[],\\n\",\n",
    "    \"    'c_len':[],\\n\",\n",
    "    \"    'category':[]\\n\",\n",
    "    \"}\\n\",\n",
    "    \"for image in data_test:    \\n\",\n",
    "    \"    #Populate cells_df dict\\n\",\n",
    "    \"    for box in image['objects']:\\n\",\n",
    "    \"        cells_test_df['path'].append(image['image']['pathname'])\\n\",\n",
    "    \"        cells_test_df['min_r'].append(box['bounding_box']['minimum']['r'])\\n\",\n",
    "    \"        cells_test_df['min_c'].append(box['bounding_box']['minimum']['c'])\\n\",\n",
    "    \"        cells_test_df['max_r'].append(box['bounding_box']['maximum']['r'])\\n\",\n",
    "    \"        cells_test_df['max_c'].append(box['bounding_box']['maximum']['c'])\\n\",\n",
    "    \"        cells_test_df['r_len'].append(box['bounding_box']['maximum']['r']-box['bounding_box']['minimum']['r'])\\n\",\n",
    "    \"        cells_test_df['c_len'].append(box['bounding_box']['maximum']['c']-box['bounding_box']['minimum']['c'])\\n\",\n",
    "    \"        cells_test_df['category'].append(box['category'])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 5,\n",
    "   \"id\": \"4bfbad5c-ca86-4d48-adeb-2dd22de66b30\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"cells_train_df = pd.DataFrame(cells_train_df)\\n\",\n",
    "    \"cells_test_df = pd.DataFrame(cells_test_df)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 6,\n",
    "   \"id\": \"f7f7ce39-e762-434b-9cd2-04226c2f95f5\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"indexes = cells_train_df[cells_train_df.category == 'red blood cell'].index\\n\",\n",
    "    \"cells_train_df = cells_train_df.drop(indexes)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 7,\n",
    "   \"id\": \"e14b19eb-5e82-4129-8179-fdf58470313a\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"indexes = cells_test_df[cells_test_df.category == 'red blood cell'].index\\n\",\n",
    "    \"cells_test_df = cells_test_df.drop(indexes)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 9,\n",
    "   \"id\": \"1c8b9856-3e2d-4063-87d8-1f7cef402adc\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"cells_arr_train = []\\n\",\n",
    "    \"labels_train = []\\n\",\n",
    "    \"for index, row in cells_train_df.iterrows():\\n\",\n",
    "    \"    full_image = cv2.imread(f\\\"data/malaria{row['path']}\\\")\\n\",\n",
    "    \"    min_r, max_r = row['min_r'], row['max_r']\\n\",\n",
    "    \"    min_c, max_c = row['min_c'], row['max_c']\\n\",\n",
    "    \"    boxed_image = Image.fromarray(full_image[min_r:max_r, min_c:max_c, :], 'RGB').resize((200,200))\\n\",\n",
    "    \"    cells_arr_train.append(np.array(boxed_image))\\n\",\n",
    "    \"    labels_train.append(row['category'])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 11,\n",
    "   \"id\": \"c54088a7-35e4-45d6-a3e5-cfede012a0a4\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"cells_arr_test = []\\n\",\n",
    "    \"labels_test = []\\n\",\n",
    "    \"for index, row in cells_test_df.iterrows():\\n\",\n",
    "    \"    full_image = cv2.imread(f\\\"data/malaria{row['path']}\\\")\\n\",\n",
    "    \"    min_r, max_r = row['min_r'], row['max_r']\\n\",\n",
    "    \"    min_c, max_c = row['min_c'], row['max_c']\\n\",\n",
    "    \"    boxed_image = Image.fromarray(full_image[min_r:max_r, min_c:max_c, :], 'RGB').resize((200,200))\\n\",\n",
    "    \"    cells_arr_test.append(np.array(boxed_image))\\n\",\n",
    "    \"    labels_test.append(row['category'])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 12,\n",
    "   \"id\": \"dca1a126-2318-4305-9b19-c19188b24bb2\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"X_train = np.array(cells_arr_train)\\n\",\n",
    "    \"y_train = np.array(labels_train)\\n\",\n",
    "    \"X_test = np.array(cells_arr_test)\\n\",\n",
    "    \"y_test = np.array(labels_test)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 15,\n",
    "   \"id\": \"745c3828-0876-4f00-8b71-c83f56ca5ded\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.preprocessing import OneHotEncoder\\n\",\n",
    "    \"def preprocess_data(X, Y):\\n\",\n",
    "    \"    \\\"\\\"\\\"trains a convolutional neural network to classify the dataset\\\"\\\"\\\"\\n\",\n",
    "    \"    X_p = K.applications.resnet50.preprocess_input(X)\\n\",\n",
    "    \"    ohe = OneHotEncoder(sparse=False)\\n\",\n",
    "    \"    Y_p = ohe.fit_transform(Y.reshape(-1,1))\\n\",\n",
    "    \"    return X_p, Y_p\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 21,\n",
    "   \"id\": \"01cf8acf-3cf0-47f9-93d4-79f0fbd7f2d2\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"trainX = K.applications.resnet50.preprocess_input(X_train)\\n\",\n",
    "    \"testX = X_p = K.applications.resnet50.preprocess_input(X_test)\\n\",\n",
    "    \"ohe = OneHotEncoder(sparse=False)\\n\",\n",
    "    \"trainy = ohe.fit_transform(y_train.reshape(-1,1))\\n\",\n",
    "    \"testy = ohe.transform(y_test.reshape(-1,1))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 22,\n",
    "   \"id\": \"9e15ee80-ab13-4791-a901-bc351db1ad45\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"inputs = K.Input(shape=(200, 200, 3))\\n\",\n",
    "    \"#Loading the ResNet50 model with pre-trained ImageNet weights\\n\",\n",
    "    \"resnet = K.applications.ResNet50(weights='imagenet',include_top=False,input_tensor=inputs)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 23,\n",
    "   \"id\": \"78bdaa16-2bac-4bb5-898e-b30c1abf2869\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"for layer in resnet.layers[:170]:\\n\",\n",
    "    \"   layer.trainable = False\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 24,\n",
    "   \"id\": \"a5830123-ed0f-4080-9c81-2aad8d59feb8\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"model = K.models.Sequential()\\n\",\n",
    "    \"model.add(K.layers.Lambda(lambda x: tf.image.resize(x,(200, 200))))\\n\",\n",
    "    \"model.add(resnet)\\n\",\n",
    "    \"model.add(K.layers.GlobalAveragePooling2D())\\n\",\n",
    "    \"model.add(K.layers.BatchNormalization())\\n\",\n",
    "    \"model.add(K.layers.Dense(256, activation='relu'))\\n\",\n",
    "    \"model.add(K.layers.BatchNormalization())\\n\",\n",
    "    \"model.add(K.layers.Dense(128, activation='relu'))\\n\",\n",
    "    \"model.add(K.layers.Dropout(0.3))\\n\",\n",
    "    \"model.add(K.layers.BatchNormalization())\\n\",\n",
    "    \"model.add(K.layers.Dense(64, activation='relu'))\\n\",\n",
    "    \"model.add(K.layers.Dropout(0.3))\\n\",\n",
    "    \"model.add(K.layers.BatchNormalization())\\n\",\n",
    "    \"model.add(K.layers.Dense(6, activation='softmax'))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 25,\n",
    "   \"id\": \"baae7dd8-1a8d-47ec-908a-b821711b172e\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\\n\",\n",
    "      \"2023-03-13 17:47:56.858043: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1292640000 exceeds 10% of free system memory.\\n\",\n",
    "      \"2023-03-13 17:47:58.311704: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1292640000 exceeds 10% of free system memory.\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Epoch 1/10\\n\",\n",
    "      \"84/85 [============================>.] - ETA: 0s - loss: 0.6315 - accuracy: 0.5104\\n\",\n",
    "      \"Epoch 1: val_accuracy improved from -inf to 0.72078, saving model to cifar10.h5\\n\",\n",
    "      \"85/85 [==============================] - 19s 133ms/step - loss: 0.6316 - accuracy: 0.5098 - val_loss: 0.4258 - val_accuracy: 0.7208\\n\",\n",
    "      \"Epoch 2/10\\n\",\n",
    "      \"84/85 [============================>.] - ETA: 0s - loss: 0.4253 - accuracy: 0.6670\\n\",\n",
    "      \"Epoch 2: val_accuracy did not improve from 0.72078\\n\",\n",
    "      \"85/85 [==============================] - 9s 102ms/step - loss: 0.4252 - accuracy: 0.6669 - val_loss: 0.3764 - val_accuracy: 0.6136\\n\",\n",
    "      \"Epoch 3/10\\n\",\n",
    "      \"84/85 [============================>.] - ETA: 0s - loss: 0.2757 - accuracy: 0.7340\\n\",\n",
    "      \"Epoch 3: val_accuracy did not improve from 0.72078\\n\",\n",
    "      \"85/85 [==============================] - 9s 103ms/step - loss: 0.2756 - accuracy: 0.7341 - val_loss: 0.3092 - val_accuracy: 0.6266\\n\",\n",
    "      \"Epoch 4/10\\n\",\n",
    "      \"84/85 [============================>.] - ETA: 0s - loss: 0.2035 - accuracy: 0.7731\\n\",\n",
    "      \"Epoch 4: val_accuracy did not improve from 0.72078\\n\",\n",
    "      \"85/85 [==============================] - 9s 101ms/step - loss: 0.2037 - accuracy: 0.7724 - val_loss: 0.2846 - val_accuracy: 0.6688\\n\",\n",
    "      \"Epoch 5/10\\n\",\n",
    "      \"84/85 [============================>.] - ETA: 0s - loss: 0.1682 - accuracy: 0.8106\\n\",\n",
    "      \"Epoch 5: val_accuracy did not improve from 0.72078\\n\",\n",
    "      \"85/85 [==============================] - 8s 99ms/step - loss: 0.1684 - accuracy: 0.8106 - val_loss: 0.3124 - val_accuracy: 0.6396\\n\",\n",
    "      \"Epoch 6/10\\n\",\n",
    "      \"84/85 [============================>.] - ETA: 0s - loss: 0.1351 - accuracy: 0.8512\\n\",\n",
    "      \"Epoch 6: val_accuracy did not improve from 0.72078\\n\",\n",
    "      \"85/85 [==============================] - 8s 98ms/step - loss: 0.1355 - accuracy: 0.8511 - val_loss: 0.5254 - val_accuracy: 0.3864\\n\",\n",
    "      \"Epoch 7/10\\n\",\n",
    "      \"84/85 [============================>.] - ETA: 0s - loss: 0.1091 - accuracy: 0.8884\\n\",\n",
    "      \"Epoch 7: val_accuracy did not improve from 0.72078\\n\",\n",
    "      \"85/85 [==============================] - 9s 101ms/step - loss: 0.1097 - accuracy: 0.8879 - val_loss: 0.3274 - val_accuracy: 0.6331\\n\",\n",
    "      \"Epoch 8/10\\n\",\n",
    "      \"84/85 [============================>.] - ETA: 0s - loss: 0.0827 - accuracy: 0.9234\\n\",\n",
    "      \"Epoch 8: val_accuracy did not improve from 0.72078\\n\",\n",
    "      \"85/85 [==============================] - 8s 97ms/step - loss: 0.0835 - accuracy: 0.9224 - val_loss: 0.4170 - val_accuracy: 0.5714\\n\",\n",
    "      \"Epoch 9/10\\n\",\n",
    "      \"84/85 [============================>.] - ETA: 0s - loss: 0.0672 - accuracy: 0.9427\\n\",\n",
    "      \"Epoch 9: val_accuracy did not improve from 0.72078\\n\",\n",
    "      \"85/85 [==============================] - 8s 97ms/step - loss: 0.0673 - accuracy: 0.9424 - val_loss: 0.3657 - val_accuracy: 0.6006\\n\",\n",
    "      \"Epoch 10/10\\n\",\n",
    "      \"84/85 [============================>.] - ETA: 0s - loss: 0.0563 - accuracy: 0.9550\\n\",\n",
    "      \"Epoch 10: val_accuracy did not improve from 0.72078\\n\",\n",
    "      \"85/85 [==============================] - 8s 98ms/step - loss: 0.0564 - accuracy: 0.9547 - val_loss: 0.7156 - val_accuracy: 0.3896\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"<keras.callbacks.History at 0x7fb66c13ae10>\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 25,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"model.compile(loss='binary_crossentropy',\\n\",\n",
    "    \"     optimizer=K.optimizers.RMSprop(lr=2e-5), metrics=['accuracy'])\\n\",\n",
    "    \"checkpointer = K.callbacks.ModelCheckpoint(filepath='cifar10.h5',\\n\",\n",
    "    \"     monitor=\\\"val_accuracy\\\", verbose=1, save_best_only=True)\\n\",\n",
    "    \"model.fit(trainX, trainy, batch_size=32, epochs=10, verbose=1,\\n\",\n",
    "    \"     callbacks=[checkpointer],validation_data=(testX, testy), shuffle=True)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"f3a592ff-2d73-47e8-b73b-48a9073643d3\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"b5a6f46a-ff3a-4b98-980a-b3d5a688c231\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"environment\": {\n",
    "   \"kernel\": \"python3\",\n",
    "   \"name\": \"pytorch-gpu.1-13.m103\",\n",
    "   \"type\": \"gcloud\",\n",
    "   \"uri\": \"gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m103\"\n",
    "  },\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.7.12\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed97cb8b-3436-48ef-bf96-d25a5cd7cdfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
